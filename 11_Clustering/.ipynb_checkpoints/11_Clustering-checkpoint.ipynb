{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd61afc",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- Clustering\n",
    "   - Evaluate cluster tendency - Hopkins statistic (*) ⚠️\n",
    "   - The k-means algorithm\n",
    "   - External evaluation metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe6c43",
   "metadata": {},
   "source": [
    "(*) ⚠️ an implementation for the Hopkins statistic is available in python (`pyclustertend`) but it requires version python < 3.10\n",
    "\n",
    "To properly execute this notebook you may need to create a novel environment as follows:\n",
    "\n",
    "```shell\n",
    "$ conda create --name dmml-clustering python=3.8 \n",
    "$ conda activate dmml-cluster\n",
    "(dmml-cluster) $ pip install pyclustertend\n",
    "(dmml-cluster) $ conda install jupyter\n",
    "```\n",
    "    # ... possibly install any other dependancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1e25de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "X, y = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=2,\n",
    "    centers=4,\n",
    "    cluster_std=(1),\n",
    "    center_box=(-10.0, 10.0),\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    ")  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea9d991",
   "metadata": {},
   "source": [
    "## Evaluate cluster tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b8a7a",
   "metadata": {},
   "source": [
    "The best way to assess cluster tendency is through visualization (when possible, i.e. dimensionality <= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8600d662",
   "metadata": {},
   "source": [
    "Is there a cluster tendency? How many clusters are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "plt.scatter(X[:,0],X[:,1], c = y)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3e1cf",
   "metadata": {},
   "source": [
    "How many clusters are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf0796",
   "metadata": {},
   "source": [
    "We can still assess cluster tendency also through the Hopkins statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11287d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyclustertend as pyct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591b885",
   "metadata": {},
   "source": [
    "Look at the docstring of hopkins statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c520d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyct.hopkins?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a047cad",
   "metadata": {},
   "source": [
    "From theory we would expect that a **value close to 1 tends to express a high cluster tendency**.\n",
    "\n",
    "However, this module refers to a different implementation.\n",
    "Look at the source code:\n",
    "    \n",
    "```python\n",
    "[...]\n",
    "x = sum(data_frame_sample_distances_to_nearest_neighbours)\n",
    "y = sum(uniformly_df_distances_to_nearest_neighbours)\n",
    "return x / (x + y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e2b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sampling_size in range(10,51,10):\n",
    "    print(f'sampled {sampling_size}: {pyct.hopkins(X,sampling_size)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa24340f",
   "metadata": {},
   "source": [
    "The low value of H confirms the *cluster* tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c19d9",
   "metadata": {},
   "source": [
    "## The k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d6070",
   "metadata": {},
   "source": [
    "In the following we apply the $k$-means algorithm to find groups in the dataset.\n",
    "Specifically,\n",
    "- we evaluate the sensitivity of the $k$-means algorithm with respect to its most influential input parameter, that is the number of clusters.\n",
    "- for each clustering evaluation, we display the clustering result as a scatter plot and compute an internal metric (silhouette score) and the value of the cost function (inertia).\n",
    "- we analyze the trend of such quantities as a function of the number of clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70dc67d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "silhouette_list = []\n",
    "inertia_list=[]\n",
    "f,axes = plt.subplots(2,4,figsize = (20,10))\n",
    "for n_clusters in range(2,10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    y_pred = kmeans.fit_predict(X)\n",
    "    \n",
    "    # evaluate silhouette score\n",
    "    silhouette_avg = silhouette_score(X,y_pred)\n",
    "    silhouette_list.append(silhouette_avg)\n",
    "\n",
    "    # evaluate inertia\n",
    "    inertia = kmeans.inertia_\n",
    "    inertia_list.append(inertia)\n",
    "    \n",
    "    # display clustered samples\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].scatter(X[:,0],X[:,1], c = y_pred,alpha = 0.5)\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].axis('equal')\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].set_xlabel('Feature 1')\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].set_ylabel('Feature 2')\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].set_title(f'k={n_clusters} - Avg. Silhouette={silhouette_avg:.2} - n_iter = {kmeans.n_iter_}' )\n",
    "\n",
    "    # display clusters centroids\n",
    "    centers = kmeans.cluster_centers_\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].scatter(centers[:,0],centers[:,1], marker = 'x',c = 'r')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# plot silhouette and inertia trends w.r.t the number of clusters\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('avg-silhouette', color='black')\n",
    "ax1.plot(range(2,10),silhouette_list,'--ok')\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "ax1.grid(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "ax2.set_ylabel('loss', color='red')  \n",
    "ax2.plot(range(2,10),inertia_list,'--or',alpha = 0.2)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c74e26",
   "metadata": {},
   "source": [
    "### Note-1: What if the two features are on different scales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c01d17",
   "metadata": {},
   "source": [
    "The kmeans \"inertia\" makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb71ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aniso=X.copy()*[4,1]\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.scatter(X_aniso[:,0],X_aniso[:,1], c = y)\n",
    "plt.axis('equal') # force equal axes aspect ratio\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bef92f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "silhouette_list = []\n",
    "inertia_list = []\n",
    "f1,axes1 = plt.subplots(2,4,figsize = (20,10))\n",
    "f2,axes2 = plt.subplots(2,4,figsize = (20,10))\n",
    "for n_clusters in range(2,10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    y_pred = kmeans.fit_predict(X_aniso)\n",
    "\n",
    "    inertia = kmeans.inertia_\n",
    "    inertia_list.append(inertia)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(X,y_pred)\n",
    "    silhouette_list.append(silhouette_avg)\n",
    "\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # NOT force equal axes aspect ratio\n",
    "    axes1[(n_clusters-2)//4][(n_clusters-2)%4].scatter(X_aniso[:,0],X_aniso[:,1], c = y_pred,alpha = 0.5)\n",
    "    axes1[(n_clusters-2)//4][(n_clusters-2)%4].set_xlabel('Feature 1')\n",
    "    axes1[(n_clusters-2)//4][(n_clusters-2)%4].set_ylabel('Feature 2')\n",
    "    axes1[(n_clusters-2)//4][(n_clusters-2)%4].set_title(f'k={n_clusters} - Avg. Silhouette={silhouette_avg:.2} - n_iter = {kmeans.n_iter_}' )\n",
    "    axes1[(n_clusters-2)//4][(n_clusters-2)%4].scatter(centers[:,0],centers[:,1], marker = 'x',c = 'r')\n",
    "\n",
    "    \n",
    "    # force equal axes aspect ratio\n",
    "    axes2[(n_clusters-2)//4][(n_clusters-2)%4].scatter(X_aniso[:,0],X_aniso[:,1], c = y_pred,alpha = 0.5)\n",
    "    axes2[(n_clusters-2)//4][(n_clusters-2)%4].axis('equal')\n",
    "    axes2[(n_clusters-2)//4][(n_clusters-2)%4].set_xlabel('Feature 1')\n",
    "    axes2[(n_clusters-2)//4][(n_clusters-2)%4].set_ylabel('Feature 2')\n",
    "    axes2[(n_clusters-2)//4][(n_clusters-2)%4].set_title(f'k={n_clusters} - Avg. Silhouette={silhouette_avg:.2} - n_iter = {kmeans.n_iter_}' )\n",
    "    axes2[(n_clusters-2)//4][(n_clusters-2)%4].scatter(centers[:,0],centers[:,1], marker = 'x',c = 'r')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "f1.set_tight_layout(True)\n",
    "f2.set_tight_layout(True)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('avg-silhouette', color='black')\n",
    "ax1.plot(range(2,10),silhouette_list,'--ok')\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "ax1.grid(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "ax2.set_ylabel('loss', color='red')  \n",
    "ax2.plot(range(2,10),inertia_list,'--or',alpha = 0.2)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497447d2",
   "metadata": {},
   "source": [
    "As the two features have different scales, the larger one will dominate the other and be the only driver of the distance evaluation. \n",
    "\n",
    "If this behaviour is unwanted, we should opt for scaling our data before clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58033f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X_aniso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2426f76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "silhouette_list = []\n",
    "inertia_list = []\n",
    "f,axes = plt.subplots(2,4,figsize = (20,10))\n",
    "for n_clusters in range(2,10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    y_pred = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    inertia = kmeans.inertia_\n",
    "    inertia_list.append(inertia)  \n",
    "    \n",
    "    silhouette_avg = silhouette_score(X,y_pred)\n",
    "    silhouette_list.append(silhouette_avg)\n",
    "\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].scatter(X_scaled[:,0],X_scaled[:,1], c = y_pred,alpha = 0.5)\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].axis('equal')\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].set_xlabel('Feature 1')\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].set_ylabel('Feature 2')\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].set_title(f'k={n_clusters} - Avg. Silhouette={silhouette_avg:.2} - n_iter = {kmeans.n_iter_}' )\n",
    "    axes[(n_clusters-2)//4][(n_clusters-2)%4].scatter(centers[:,0],centers[:,1], marker = 'x',c = 'r')\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('avg-silhouette', color='black')\n",
    "ax1.plot(range(2,10),silhouette_list,'--ok')\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "ax1.grid(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "ax2.set_ylabel('loss', color='red')  \n",
    "ax2.plot(range(2,10),inertia_list,'--or',alpha = 0.2)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0a73a",
   "metadata": {},
   "source": [
    "### Note-2: What is the impact of centroids initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc01f5",
   "metadata": {},
   "source": [
    "By default, sklearn.cluster.KMeans uses the **$k$-means++** initialization strategy: it initializes the centroids to be (generally) distant from each other, leading to probably better results than random initialization.\n",
    "\n",
    "Furthermore, by default the clustering procedure is iterated with `n_init=10` different random centroid seeds: the final results will be the best output of n_init consecutive runs in terms of inertia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c577c10",
   "metadata": {},
   "source": [
    "In the following we run \n",
    "- 100 times, n_init = 1, random initialization, n_cluster = 4\n",
    "- 100 times, n_init = 1, $k$-means++ initialization, n_cluster = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sil_random = [silhouette_score(X,KMeans(n_clusters = 4, init = 'random', random_state=i, n_init=1).fit_predict(X)) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sil_plusplus = [silhouette_score(X,KMeans(n_clusters = 4, init = 'k-means++', random_state=i, n_init=1).fit_predict(X)) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725bed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'random':avg_sil_random,'km++':avg_sil_plusplus})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a99d9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80675c54",
   "metadata": {},
   "source": [
    "Random initialization entails lower performance, achieving frequently lower values of average silhouette score.\n",
    "\n",
    "Let's see some poor result due to (poor) random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb8be7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.figure()\n",
    "    kmeans = KMeans(n_clusters = 4, init = 'random', random_state=i, n_init=1)\n",
    "    y_pred = kmeans.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X,y_pred)\n",
    "    plt.scatter(X[:,0],X[:,1], c = y_pred,alpha = 0.5)\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(f'k=4 - Avg. Silhouette={silhouette_avg:.2} - n_iter = {kmeans.n_iter_} - seed={i}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c83c37",
   "metadata": {},
   "source": [
    "# External evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2519ba",
   "metadata": {},
   "source": [
    "How to evaluate the result of a clustering algorithm:\n",
    "- through **internal (or intrinsic) metrics**: quality of the modelling itself (compactness and separation), without any ground truth labels (e.g., silhouette score)\n",
    "- through **external (or extrinsic) metrics**: quality of the modelling based on ground truth labels (e.g., BCubed Precision, BCubed Recall, Adjusted Rand Index)\n",
    "\n",
    "The whole list of metrics available in scikit-learn is reported [here](https://scikit-learn.org/stable/modules/classes.html#clustering-metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecff93d",
   "metadata": {},
   "source": [
    "Looking at the trend of average silhouette score, two *peaks* are reached for n_clusters = 2 and n_clusters = 4. The two clustering actually reflects the coarse-grained and fine-grained structure of the dataset, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14941c75",
   "metadata": {},
   "source": [
    "The availability of ground truth labels enables an additional evaluation, through the so-called **external metrics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=10)\n",
    "y_pred_k4 = kmeans.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=10)\n",
    "y_pred_k2 = kmeans.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5949b6",
   "metadata": {},
   "source": [
    "**Adjusted Rand Index (ARI)**\n",
    "Rand index adjusted for chance.\n",
    "\n",
    "The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "\n",
    "- ARI = 0: random labelling\n",
    "- ARI = 1: perfect agreement between the two clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score as ARI\n",
    "print(f'kmeans with n_cluster = 4 --> ARI = {ARI(y,y_pred_k4):.03}')\n",
    "print(f'kmeans with n_cluster = 2 --> ARI = {ARI(y,y_pred_k2):.03}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8f112",
   "metadata": {},
   "source": [
    "**Contingency matrix**\n",
    "- $C_{ij}$ represents the number of samples of class $i$ that are in cluster $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix as CM\n",
    "CM(y,y_pred_k4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix as CM\n",
    "CM(y,y_pred_k2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afebfa61",
   "metadata": {},
   "source": [
    "**homogeneity, completeness, v-measure**\n",
    "- *homogeneity*: a property that is satisfied if each cluster contains only members of a single class.\n",
    "- *completeness*: a property that is satisfied if all members of a given class are assigned to the same cluster.\n",
    "- *V-measure*: harmonic mean of homogeneity score and completeness score\n",
    "\n",
    "Note that function homogeneity_score and completeness_score are not symmetric: switching *y_true* with *y_pred* will return the completeness_score and homogeneity_score, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec276cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import homogeneity_completeness_v_measure as HCV\n",
    "homogeneity_k2, completeness_k2, v_measure_k2 = HCV(y, y_pred_k2)\n",
    "print(f'homogeneity_k2: {homogeneity_k2}')\n",
    "print(f'completeness_k2: {completeness_k2}')\n",
    "print(f'v_measure_k2: {v_measure_k2}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca4886",
   "metadata": {},
   "source": [
    "- *low homogeneity*: looking at the contingency matrix, we notice that the first cluster (first column) gather objects from three different classes.\n",
    "\n",
    "- *perfect completeness*: all objects from a given class are assigned to the same cluster (class 1 to cluster 2, class 2-3-4 to cluster 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import homogeneity_completeness_v_measure as HCV\n",
    "homogeneity_k4, completeness_k4, v_measure_k4 = HCV(y, y_pred_k4)\n",
    "print(f'homogeneity_k4: {homogeneity_k4}')\n",
    "print(f'completeness_k4: {completeness_k4}')\n",
    "print(f'v_measure_k4: {v_measure_k4}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd6b63",
   "metadata": {},
   "source": [
    "# <font color='blue'><ins>TASK</ins></font>\n",
    "- Carry out a **clustering analysis** considering the following setting.\n",
    "    - Apply agglomerative clustering on exactly the same dataset (synthetic, originated with *make_blobs*).\n",
    "    - Discuss the dependancy of the clustering results upon the linkage criterion used\n",
    "    - For each linkage value\n",
    "        - plot the *dendrogram* (as usual, check the resources available in sklearn)\n",
    "        - set n_clusters = 4 and\n",
    "            - display the clustering result as a scatter plot.\n",
    "            - evaluate the clustering results in terms of one or more external metrics. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
