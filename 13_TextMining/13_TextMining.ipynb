{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "- The 20newsgroup dataset \n",
    "- Text vectorization: numeric representation of text\n",
    "- Classification model\n",
    "- Pipelines for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 20newsgroup dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the 20newsgroups dataset by exploiting the dedicated function in `sklearn.datasets` module. (what is a [newsgroup](https://en.wikipedia.org/wiki/Usenet_newsgroup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "ng20_train = fetch_20newsgroups(subset='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(ng20_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for other \"native\" sklearn datasets, the 20newsgroups is provided as a `Bunch` data structure and has the following attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(ng20_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ng20_train.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng20_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ng20_train.data),len(ng20_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng20_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_n = pd.Series(ng20_train.target).apply(lambda x: ng20_train.target_names[x])\n",
    "target_vc = pd.Series(target_n).value_counts()\n",
    "target_vc.plot(kind='bar',title = '20newsgroups train: classes distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng20_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most classes have around 600 samples. Few classes are slightly less represented but in any case with more than 300 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in [0, 123,2000]:\n",
    "    print(f'\\n\\n\\t\\t RECORD {idx}')\n",
    "    print(f'category index: {ng20_train.target[idx]} - name: {ng20_train.target_names[ng20_train.target[idx]]}')\n",
    "    print()\n",
    "    print(ng20_train.data[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the histogram of the length of the pieces of text (in terms of number of characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist([len(x) for x in ng20_train.data],bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is possible to load only a sub-selection of the categories by passing the list of the categories to load to the `fetch_20newsgroups` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng4_train = fetch_20newsgroups(subset='train',categories = categories)\n",
    "print(len(ng4_train.data))\n",
    "print(ng4_train.target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in [0, 10, 20]:\n",
    "    print(f'\\n\\n\\t\\t RECORD {idx}')\n",
    "    print(f'category index: {ng4_train.target[idx]} - name: {ng4_train.target_names[ng4_train.target[idx]]}')\n",
    "    print()\n",
    "    print(ng4_train.data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_n = pd.Series(ng4_train.target).apply(lambda x: ng4_train.target_names[x])\n",
    "target_vc = pd.Series(target_n).value_counts()\n",
    "target_vc.plot(kind='bar',title = '4newsgroups train: classes distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization: numeric representation of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning algorithms we will use require us to give numerical data to them. Raw text data as an input will not work! This means that we have to transfer our texts to some kind of numerical representation without loosing too much information. Transferring a text from a sequence of characters to a vector of numbers is called **text vectorization**.\n",
    "\n",
    "![text_vectorization.png](images/text_vectorization.png)\n",
    "\n",
    "There are many different ways to vectorize texts, from fancy techniques like [word embeddings](https://en.wikipedia.org/wiki/Word_embedding) and topic models like [latent dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA) to simple [bag-of-words models](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "The most intuitive way to turn the text content into numerical feature vectors is the **bag of words representation**:\n",
    "\n",
    "* assign a **fixed integer id** to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
    "\n",
    "* for each document $d_i$, count the **number of occurrences** of each word $w$ and store it in $X[i, j]$ as the value of feature $w_j$ where $j$ is the index of word $w$ in the dictionary.\n",
    "\n",
    "The bag of words representation implies that `n_features` is the number of distinct words in the corpus: this number is typically very large ($10^5, 10^6$).\n",
    "\n",
    "Let the number of features be $100,000$. If `n_samples` is $10,000$, storing `X` as a NumPy array of type `float32` would require $10,000 \\times 100,000 \\times 4$ bytes = 4 GB in RAM which may still be manageable on today’s computers, But undoubtedly burdensome.\n",
    "\n",
    "Fortunately, most values in `X` will be zeros since for a given document less than a couple thousands of distinct words will be used. For this reason we say that bag of words are typically *high-dimensional sparse datasets*. We can save a lot of memory by *only storing the non-zero parts of the feature vectors in memory*.\n",
    "The `scipy.sparse` matrices are data structures that do exactly this, and scikit-learn has built-in support for these structures.\n",
    "\n",
    "`scikit-learn` offers a provides basic tools to process text using the bag of words representation. To build such a representation we will proceed as follows:\n",
    "\n",
    "* *tokenize* strings and *give an integer id* for each possible token, for instance by using whitespaces and punctuation as token separators.\n",
    "* *count* the occurrences of tokens in each document.\n",
    "* *normalize* and weighting with diminishing importance tokens that occur in the majority of samples (i.e. documents).\n",
    "\n",
    "![tfidf_vectorization.png](images/tfidf_vectorization.png)\n",
    "\n",
    "This approach is called [**TFIDF**](http://en.wikipedia.org/wiki/Tf–idf):\n",
    "\n",
    "* **term frequency** (TF): counts the number of times a term $t$ (word) appears in a document $d$ adjusted by the length of the document (number of all words $t'$ in document $d$).\n",
    "* **inverse document frequency** (IDF): counts the number of documents $n_t$ an individual term $t$ appears over all documents $N$.\n",
    "* **term frequency-inverse document frequency** (TFIDF): weights down common words like \"the\" and gives more weight to rare words like \"software\".\n",
    "\n",
    "To perform this vectorization, scikit-learn provides the `TfidfVectorizer` class.\n",
    "\n",
    "Usage of `TfidfVectorizer` is equivalent to usage of `CountVectorizer` (*tokenize* and *count*) followed by `TfidfTransformer` (*normalize*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects enable many operations. Let's have a look at the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class sklearn.feature_extraction.text.CountVectorizer(*,\n",
    "  input='content', \n",
    "  encoding='utf-8', \n",
    "  decode_error='strict', \n",
    "  strip_accents=None,   # Remove accents and perform character normalization, (by default does nothing)\n",
    "  lowercase=True, \n",
    "  preprocessor=None,  # Override strip_accents and lowercase while preserving tokenizing and n-grams\n",
    "  tokenizer=None, # Override the string tokenization step while preserving the preprocessing and n-grams generation steps. \n",
    "  stop_words=None, # If None, no stop words will be used. \n",
    "  token_pattern='(?u)\\b\\w\\w+\\b', # Regular expression denoting what constitutes a \"token\"\n",
    "  ngram_range=(1, 1),  # (lower, upper) boundary of the values of n in n-grams. (1,2) -> unigrams and bigrams\n",
    "  analyzer='word', # Whether the feature should be made of word n-gram or character n-grams.  If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n",
    "  max_df=1.0,  # Build vocabulary ignoring terms that have a document frequency higher than the threshold (corpus-specific stop words)\n",
    "  min_df=1,  # Build vocabulary ignoring terms that have a document frequency lower than the threshold (cut-off)\n",
    "  max_features=None,# If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "  vocabulary=None, # Custom vocabulary, otherwise determined from input documents\n",
    "  binary=False, # If True, all non zero counts are set to 1. \n",
    "  dtype=<class 'numpy.int64'>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vector = count_vect.fit_transform(ng4_train.data)\n",
    "X_train_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse matrix has 2257 rows (number of documents) and 35644 columns (number of tokens in vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word occurrences of a document**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_vect.transform(['the car is red']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_vect.transform(['the car is red. The cat is red as well']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_vect.transform(['la macchina è rossa']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the vocabulary the vectorizer learned from our data. We call the `vocabulary_` property on the trained vectorizer to retrieve the full vocabulary and then use a little loop to print the first items in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = count_vect.vocabulary_ # it is a mapping of terms to feature indices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['the', 'car', 'is', 'red', 'the', 'cat', 'is', 'red', 'as', 'well']:\n",
    "    print(f'{x}: {vocabulary.get(x,\"token not in voc\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['la', 'macchina', 'è', 'rossa']:\n",
    "    print(f'{x}: {vocabulary.get(x,\"token not in voc\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary = count_vect.vocabulary_\n",
    "\n",
    "#little loop to print the first items in the vocabulary\n",
    "for count, item in enumerate(iter(vocabulary.items())):\n",
    "    print(item)\n",
    "    if count >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The vocabulary contains {len(vocabulary)} terms in total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = count_vect.get_feature_names_out()\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[10000:10100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse on the features that are extracted (i.e. tokens) suggests that there are a lot of meaningless  words.\n",
    "We can perform a more aggressive preprocessing (e.g., removing numbers and other special symbols) to avoid this.\n",
    "\n",
    "\n",
    "Furthermore, by setting the parameters `max_df` and/or `min_df` and/or `max_features`, we can further reduce the number of words (i.e. number of attributes) in order to:\n",
    "- apply a sort of feature selection (remove noisy features)\n",
    "- reduce memory consumption \n",
    "- reduce required computational power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_vector)\n",
    "\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_transformer.transform(count_vect.transform(['the car is red'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine `CountVectorizer` and `TfIdfTransformer` in a single object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords)\n",
    "X_train_tfidf = vectorizer.fit_transform(ng4_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform(['the car is red']).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf.nnz / float(X_train_tfidf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "123/35644"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, there are 124 non-zero components by sample in a 35644-dimensional space (0.035%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the files pertaining to the same 5 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng4_test = fetch_20newsgroups(subset='test',categories = categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_n = pd.Series(ng4_test.target).apply(lambda x: ng4_test.target_names[x])\n",
    "target_vc = pd.Series(target_n).value_counts()\n",
    "target_vc.plot(kind='bar',title = '4newsgroups test: classes distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ng4_test.data), len(ng4_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, ng4_train.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novel, custom, samples (you can simulate *production* behaviour)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['is there any afterlife', 'OpenGL on the GPU is fast']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new_count = vectorizer.transform(docs_new)\n",
    "docs_new_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = clf.predict(docs_new_count)\n",
    "[ng4_train.target_names[x] for x in predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_prob = clf.predict_proba(docs_new_count)\n",
    "predicted_class_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The actual test-set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = vectorizer.transform(ng4_test.data)\n",
    "y_pred = clf.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,ConfusionMatrixDisplay\n",
    "print(classification_report(ng4_test.target,y_pred,target_names = ng4_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(ng4_test.target,y_pred,display_labels = ng4_test.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy is quite high (around 90%), and so are precision and recall per class.\n",
    "\n",
    "The only exceptions arise for the **precision of class \"soc.religion.chrisitan\"** (0.75) and **recall of class \"alt.atheism\"**. Specifically 80 examples from class alt.atheism are improperly classified as soc.religion.chrisitan.\n",
    "Evidently, the two classes are likely to pertain to similar semantic areas and this introduces some confusion in discriminating among them.\n",
    "\n",
    "Let's have a look at some misclassified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_atheism = np.asarray(ng4_test.data)[(ng4_test.target!=y_pred) & (ng4_test.target == 0)]\n",
    "for m_atheism in misclassified_atheism[:3]:\n",
    "    print(m_atheism)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we get some information about the most informative features?\n",
    "\n",
    "The MultinomialNB object exposes the attribute `feature_log_prob_` which represents the empirical log probability of features given a class, $P(x_i|y)$.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
    "import numpy as np\n",
    "def show_top10(clf, vectorizer, categories):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(clf.feature_log_prob_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "show_top10(clf, vectorizer, ng4_train.target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice, for example, that among the most informative features there are some apparently unrelated to the topic (e.g. *edu*, *com*) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text processing and classification is a typical scenario where pipelines can be helpful.\n",
    "\n",
    "In the following, the whole exercise is repeated for the whole 20newsgroup dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "ng20_train = fetch_20newsgroups(subset='train')\n",
    "ng20_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words = stopwords)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(ng20_train.data,ng20_train.target)\n",
    "y_pred = text_clf.predict(ng20_test.data)\n",
    "print(classification_report(ng20_test.target,y_pred,target_names = ng20_test.target_names))\n",
    "fig,ax = plt.subplots(figsize = (15,15))\n",
    "ConfusionMatrixDisplay.from_predictions(ng20_test.target,y_pred,display_labels = ng20_test.target_names,ax=ax,xticks_rotation='vertical')\n",
    "plt.show()\n",
    "len(text_clf[0].vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to add the stemmer to the text processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = TfidfVectorizer(stop_words = stopwords).build_analyzer()\n",
    "print(analyzer(ng20_train.data[0])[:10])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer('english')\n",
    "def snowball_analyzer(doc):\n",
    "    return [snow_stemmer.stem(w) for w in analyzer(doc)]\n",
    "print(snowball_analyzer(ng20_train.data[0])[:10])\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer = snowball_analyzer)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.fit(ng20_train.data,ng20_train.target)\n",
    "y_pred = text_clf.predict(ng20_test.data)\n",
    "print(classification_report(ng20_test.target,y_pred,target_names = ng20_test.target_names))\n",
    "fig,ax = plt.subplots(figsize = (15,15))\n",
    "ConfusionMatrixDisplay.from_predictions(ng20_test.target,y_pred,display_labels = ng20_test.target_names,ax=ax,xticks_rotation='vertical')\n",
    "plt.show()\n",
    "len(text_clf[0].vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we comment this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'><ins>TASK</ins></font>\n",
    "- 13_TASK-1: \n",
    "    - Consider the 20newsgroup dataset, limited to the 4 categories we have analyzed in this notebook.\n",
    "    - Can you tune the processing/classification pipeline and improve the performance on the test set?\n",
    "        - For example (not exhaustive, be creative!)\n",
    "            - act on the text processing stage to reduce the feature space and remove \"noisy\" tokens;\n",
    "            - act on the text processing stage by apply stemming;\n",
    "            - remove the header (`fetch_20newsgroup` has an attribute `remove` that you can tune);\n",
    "            - extract only the \"subject\" of each piece of news and try to infer the topic based only on that; \n",
    "            - use other classification models;\n",
    "        - Note: select your best model (text processing / vectorization / classification) on the training set by using cross-validation, and then evaluate its final performance on the test set\n",
    "\n",
    "- 13_TASK-2:\n",
    "    - Consider the TripAdvisor dataset\n",
    "        - Carry out an exploratory data analysis.        \n",
    "        - Carry out a classification analysis in the following setting:\n",
    "            - consider three models, namely SVM - MultinomialNB - DecisionTree.\n",
    "            - Evaluate the classifiers by applying a 5 fold cross-validation\n",
    "            - Compare the results achieved by the three models on the test set, in terms of accuracy\n",
    "            - Comment the results achieved in terms of precision and recall per class    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
